{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " TensorFlow_FeedForward_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMwY7Ic6ncH6io7I5y1gOhg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dotdigital26/Tensorflow-Implementation-on-Mnist-Data/blob/main/TensorFlow_FeedForward_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MOD9nEeKr7L"
      },
      "source": [
        "MNIST DIGIT CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_etT2JtDfAqF",
        "outputId": "8d394dee-d7ac-447a-a223-ad7d4c02a14b"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "mnist = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7eatcISAA3wH",
        "outputId": "f3918028-4c76-49de-854b-b126549bbdb8"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist\n",
        "plt.imshow(x_train[1],cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8c8b5b2a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT5QtRKfITcL"
      },
      "source": [
        "def sigmoid(x):\n",
        "\n",
        "  x =np.clip(x, -500, 500) # This helps to limit the value of variable such that every value below min (-500) is renamed as -500 and same applies to max 50\n",
        "  \n",
        "  return np.where(x >=0, 1/(1+np.exp(x)))\n",
        "\n",
        "def dsigmoid(x):\n",
        "  return sigmoid(x) * (1-sigmoid(x))\n",
        "\n",
        "def softmax(x):\n",
        "  b = x.max()\n",
        "  y = np.exp(x - b)\n",
        "  return y/y.sum()\n",
        "\n",
        "def cross_entropy_loss(y, yHat):\n",
        "  return -np.sum(y * np.log(yHat))\n",
        "\n",
        "def integer_to_one_hot(x, max):\n",
        "  result = np.zero(10)\n",
        "  result[x] =1\n",
        "  return result\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDvCV5zqPV1O"
      },
      "source": [
        "#The next is initialization of weights, biases and model architechure. Bad weight initialization can affect our training\n",
        "#by causing no or slow convergence to the global minimum. In other to avoid that, we need to make sure our weight is initialized \n",
        "#to be closer to zero and closer to one. The whole point of this initialization(Xavier/Glorot Initialiation) is to avoid the problem of vanishing gradient which happens,\n",
        "#when the input of an activation function z=w^Tx + b is far away from the and the gradient of its update function equal 0. To avoid this, use Fan-in and Fanout or completely use \n",
        "#we can introduce RELU activation Unit\n",
        "\n",
        "import math\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng(80085)\n",
        "\n",
        "##Neural Network Layer Sizes: 784 --> 32 --> 32 --> 10\n",
        "weights =[rng.normal(0,1/math.sqrt(784),(32,784)),\n",
        "          (rng.normal(0, 1/math.sqrt(32), (32, 32)),\n",
        "           rng.normal(0, 1/math.sqrt(32), (10,32)))]\n",
        "biases = [np.zeros(32), np.zeros(32), np.zeros(10)]\n",
        "\n",
        "#Plotting histogram of layer weights to chevk probability distribution\n",
        "print('Weight distribution per layer')\n",
        "for index, layer in enumerate(weights):\n",
        "  plt.figure()\n",
        "  plt.suptitle(\n",
        "      'Layer ' + str(index + 1) + ' with ' + str(layer.shape[0]) +\n",
        "      'Neurons, ' + str(layer.shape[1]) + ' inputs each  (' + str(layer.size) +\n",
        "      'weights in total)'\n",
        "       )\n",
        "  plt.hist(layer.flatten(), bins=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot14jXDxZJAe"
      },
      "source": [
        "\n",
        "def feed_forward_sample(sample, y):\n",
        "  \"\"\" Feeds a sample forward through the neural network.\n",
        "    Parameters:\n",
        "      sample: 1D numpy array. The input sample (an MNIST digit).\n",
        "      label: An integer from 0 to 9.\n",
        "\n",
        "    Returns: The cross entropy loss.\n",
        "  \"\"\"\n",
        "  a = sample.flatten()\n",
        "\n",
        "  for index, w in enumerate(weights):\n",
        "    z = np.matmul(w, a) + biases[index]\n",
        "    if index < len(weights) - 1:\n",
        "      a = sigmoid(z)\n",
        "    else:\n",
        "      a = softmax(z)\n",
        "\n",
        "  # Calculate loss\n",
        "  one_hot_y = integer_to_one_hot(y, 10)\n",
        "  loss = cross_entropy_loss(one_hot_y, a)\n",
        "\n",
        "  # Convert activations to one hot encoded guess\n",
        "  one_hot_guess = np.zeros_like(a)\n",
        "  one_hot_guess[np.argmax(a)] = 1\n",
        "  \n",
        "  return loss, one_hot_guess\n",
        "\n",
        "\n",
        "\n",
        "#Feedforward all training samples\n",
        "def feed_forward_dataset(x,y):\n",
        "  losses = np.empty(x.shape[0])\n",
        "  one_hot_guesses = np.empty((x.shape[0], 10))\n",
        "\n",
        "  for i in range(x.shape[0]):\n",
        "    if i == 0 or ((i + 1) % 10000 == 0):\n",
        "      print(i + 1, \"/\", x.shape[0], \"(\", format(((i + 1)/x.shape[0]) * 100, \".2f\"), \"%)\")\n",
        "    losses[i], one_hot_guesses[i] = feed_forward_sample(x[i], y[i])\n",
        "\n",
        "  print(\"\\nAverage loss:\", np.round(np.average(losses), decimals=2))\n",
        "\n",
        "  y_one_hot = np.zeros((y.size, 10))\n",
        "  y_one_hot[np.arange(y.size), y] = 1\n",
        "\n",
        "  correct_guesses = np.sum(y_one_hot * one_hot_guesses)\n",
        "  correct_guesses_percent = format((correct_guesses / y.shape[0]) * 100, \".2f\")\n",
        "  print('Accuracy (# of correct guesses):', correct_guesses, \"/\", y.shape[0], \"(\",correct_guesses_percent, \"%)\")\n",
        "\n",
        "def fee_forward_training_data():\n",
        "  print(\"Feeding forward all training data...\")\n",
        "  feed_forward_dataset(x_train, y_train)\n",
        "  print(\"\")\n",
        "\n",
        "def fee_forward_test_data():\n",
        "  print(\"Feeding forward all test data...\")\n",
        "  feed_forward_dataset(x_test, y_test)\n",
        "  print(\"\")\n",
        "fee_forward_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGmlOReG2pFK"
      },
      "source": [
        "\n",
        "def train_one_sample(sample, y, learning_rate=0.003):\n",
        "  a = sample.flatten()\n",
        "\n",
        "  # We will store each layer's activations to calculate gradient\n",
        "  activations = []\n",
        "\n",
        "  # Feedforward\n",
        "  for i, w in enumerate(weights): # Each w is a layer's 2D weight matrix\n",
        "    z = np.matmul(w, a) + biases[i]\n",
        "    if (i < len(weights) - 1):\n",
        "      a = sigmoid(z)\n",
        "    else:  \n",
        "      a = softmax(z) # softmax on last layer\n",
        "    activations.append(a)\n",
        "\n",
        "  # Calculate loss\n",
        "  one_hot_y = integer_to_one_hot(y, 10)\n",
        "  loss = cross_entropy_loss(one_hot_y, a)\n",
        "\n",
        "  # Convert last layer's activations to one hot encoded guess\n",
        "  one_hot_guess = np.zeros_like(a)\n",
        "  one_hot_guess[np.argmax(a)] = 1\n",
        "\n",
        "  # Check whether guess was correct\n",
        "  correct_guess = (np.sum(one_hot_y * one_hot_guess) == 1)\n",
        "\n",
        "  weight_gradients = [None] * len(weights)\n",
        "  bias_gradients = [None] * len(weights)\n",
        "  activation_gradients = [None] * (len(weights) - 1)\n",
        "  \n",
        "  # Backpropagation\n",
        "  for i in range(len(weights) - 1, -1, -1): # Traverse layers in reverse\n",
        "    last_layer = i == len(weights) - 1\n",
        "    second_to_last_layer = i == len(weights) - 2\n",
        "\n",
        "    if last_layer:\n",
        "      # Gather all needed variables, making vectors vertical\n",
        "      y = one_hot_y[:, np.newaxis]\n",
        "      a = activations[i][:, np.newaxis]\n",
        "      a_prev = activations[i-1][:, np.newaxis]\n",
        "\n",
        "      weight_gradients[i] = np.matmul((a - y), a_prev.T)\n",
        "      bias_gradients[i] = a - y\n",
        "\n",
        "    else:\n",
        "      # Gather all needed variables, making vectors vertical\n",
        "      w_next = weights[i+1]\n",
        "      a_next = activations[i + 1][:, np.newaxis]\n",
        "      y = one_hot_y[:, np.newaxis]\n",
        "      a = activations[i][:, np.newaxis]\n",
        "      if i > 0:\n",
        "        a_prev = activations[i-1][:, np.newaxis]\n",
        "      else:\n",
        "        # Previous activation is the sample itself\n",
        "        a_prev = sample.flatten()[:, np.newaxis]\n",
        "\n",
        "      # Activation gradient\n",
        "      if second_to_last_layer:\n",
        "        dCda = np.matmul(w_next.T, (a_next - y))\n",
        "        activation_gradients[i] = dCda\n",
        "      else:\n",
        "        dCda_next = activation_gradients[i+1]\n",
        "        dCda = np.matmul(w_next.T, (dsigmoid(a_next) * dCda_next))\n",
        "        activation_gradients[i] = dCda\n",
        "\n",
        "      # Weights & biases gradients\n",
        "      x = dsigmoid(a) * dCda\n",
        "      weight_gradients[i] = np.matmul(x, a_prev.T)\n",
        "      bias_gradients[i] = x\n",
        "\n",
        "    # Update weights & biases based on gradient\n",
        "    weights[i] -= weight_gradients[i] * learning_rate\n",
        "    biases[i] -= bias_gradients[i].flatten() * learning_rate\n",
        "\n",
        "def train_one_epoch(learning_rate=0.003):\n",
        "  print(\"Training for one epoch over the training dataset...\")\n",
        "  for i in range(x_train.shape[0]):\n",
        "    if i == 0 or ((i + 1) % 10000 == 0):\n",
        "      completion_percent = format(((i + 1) / x_train.shape[0]) * 100, \".2f\")\n",
        "      print(i + 1, \"/\", x_train.shape[0], \"(\", completion_percent, \"%)\")\n",
        "    train_one_sample(x_train[i], y_train[i], learning_rate)\n",
        "  print(\"Finished training.\\n\")\n",
        "\n",
        "# Train and check accuracy before & after each epoch\n",
        "\n",
        "feed_forward_test_data()\n",
        "\n",
        "def test_and_train():\n",
        "  train_one_epoch()\n",
        "  feed_forward_test_data()\n",
        "\n",
        "for i in range(3): # Adjust number of epochs here\n",
        "  test_and_train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}